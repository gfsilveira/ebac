{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnLtRqBz7Mm5"
      },
      "source": [
        "# Módulo 24, Tarefa 02"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9oLqh-P7Mm8"
      },
      "source": [
        "## 1. Cite 5 diferenças entre o AdaBoost e o GBM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGO4qMzX7Mm9"
      },
      "source": [
        "||Caracteristica|GBM|AdaBoost|\n",
        "|-|-|-|-|\n",
        "|1|Tipo de árvores|Árvores completas com poda|Árvores com duas folhas e um de profundidade (Stump)|\n",
        "|2|Independência|Resíduo de uma árvore influência a próxima|Resultado de uma stump influência a próxima|\n",
        "|3|Pesos das previsões|Learning rate|Ponderado|\n",
        "|4|Variáveis|Cada árvore utiliza todas as variáveis|Cada stump é de uma variável|\n",
        "|5|Target|Resíduos * learning rate|Variável dependente|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB_UmP-N7Mm9"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZademFm7Mm-"
      },
      "source": [
        "## 2 Acesse o link <a href='https://scikit-learn.org/stable/modules/ensemble.html' >Scikit-learn – GBM</a>, leia a explicação (traduza se for preciso) e crie um jupyter notebook contendo o exemplo de classificação e de regressão do GBM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lel5ascg7Mm_",
        "outputId": "4b21465f-a12a-4dc1-ff7f-527570f5a5e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8965"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.datasets import make_hastie_10_2\n",
        "\n",
        "X, y = make_hastie_10_2(random_state=0)\n",
        "X_train, X_test = X[:2000], X[2000:]\n",
        "y_train, y_test = y[:2000], y[2000:]\n",
        "\n",
        "clf = HistGradientBoostingClassifier(max_iter=100).fit(X_train, y_train)\n",
        "clf.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4CxOVuP7MnG"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUZaOl2w7MnI"
      },
      "source": [
        "## 3 Cite 5 Hyperparametros importantes no GBM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXnS3esY7MnI"
      },
      "source": [
        "||Hiperparâmetro|Default|Descrição|\n",
        "|-|-|-|-|\n",
        "|1|loss|log_loss|A função de perda para usar no processo de boost.|\n",
        "|2|learning_rate|1.0|Peso aplicado a cada classificador|\n",
        "|3|max_iterint|100|O número máximo de iterações do processo de boost.|\n",
        "|4|max_leaf_node|31|O número máximo de folhas para cada árvore.|\n",
        "|5|max_depth|None|A profundidade máxima de cada árvore.|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9vcUOOK7MnJ"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsA-xQVK7MnJ"
      },
      "source": [
        "## 4  Utilize o GridSearch para encontrar os melhores hyperparametros para o conjunto de dados do exemplo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Jv7vBiC7MnK",
        "outputId": "e0ccf8d1-fd28-4244-e037-e3a05501dfd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.948889 using {'learning_rate': 0.0001, 'n_estimators': 500}\n",
            "0.666667 (0.000000) with: {'learning_rate': 0.0001, 'n_estimators': 10}\n",
            "0.666667 (0.000000) with: {'learning_rate': 0.0001, 'n_estimators': 50}\n",
            "0.666667 (0.000000) with: {'learning_rate': 0.0001, 'n_estimators': 100}\n",
            "0.948889 (0.053564) with: {'learning_rate': 0.0001, 'n_estimators': 500}\n",
            "0.666667 (0.000000) with: {'learning_rate': 0.001, 'n_estimators': 10}\n",
            "0.948889 (0.053564) with: {'learning_rate': 0.001, 'n_estimators': 50}\n",
            "0.948889 (0.053564) with: {'learning_rate': 0.001, 'n_estimators': 100}\n",
            "0.922222 (0.048939) with: {'learning_rate': 0.001, 'n_estimators': 500}\n",
            "0.948889 (0.053564) with: {'learning_rate': 0.01, 'n_estimators': 10}\n",
            "0.922222 (0.048939) with: {'learning_rate': 0.01, 'n_estimators': 50}\n",
            "0.922222 (0.048939) with: {'learning_rate': 0.01, 'n_estimators': 100}\n",
            "0.926667 (0.049740) with: {'learning_rate': 0.01, 'n_estimators': 500}\n",
            "0.922222 (0.048939) with: {'learning_rate': 0.1, 'n_estimators': 10}\n",
            "0.922222 (0.048939) with: {'learning_rate': 0.1, 'n_estimators': 50}\n",
            "0.922222 (0.048939) with: {'learning_rate': 0.1, 'n_estimators': 100}\n",
            "0.917778 (0.044500) with: {'learning_rate': 0.1, 'n_estimators': 500}\n",
            "0.944444 (0.054659) with: {'learning_rate': 1.0, 'n_estimators': 10}\n",
            "0.946667 (0.055511) with: {'learning_rate': 1.0, 'n_estimators': 50}\n",
            "0.940000 (0.062893) with: {'learning_rate': 1.0, 'n_estimators': 100}\n",
            "0.940000 (0.062893) with: {'learning_rate': 1.0, 'n_estimators': 500}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "\n",
        "# Generate synthetic classification dataset\n",
        "X, y = make_classification(n_samples=100, n_features=20, n_informative=10, n_redundant=10, random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.1, 1],\n",
        "    'max_iter': [50, 100],\n",
        "    'max_leaf_nodes': [31, 63, 127]\n",
        "}\n",
        "\n",
        "# Perform grid search\n",
        "grid_search = GridSearchCV(estimator=HistGradientBoostingClassifier(random_state=42),\n",
        "                           param_grid=param_grid,\n",
        "                           cv=5,\n",
        "                           scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Report best score and parameters\n",
        "print(f\"Best score: {grid_search.best_score_:.3f}\")\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "best_model = grid_search.best_estimator_\n",
        "accuracy = best_model.score(X_test, y_test)\n",
        "print(f\"Test set accuracy: {accuracy:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLXkVLxg7MnL"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5 Acessando o artigo do Jerome Friedman (<a href='https://jerryfriedman.su.domains/ftp/stobst.pdf' >Stochastic</a>) e pensando no nome dado ao Stochastic GBM, qual é a maior diferença entre os dois algoritmos?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Meu entendimento é que a função custo da GBM é baseada em todo o conjunto de treinamento, enquanto a GBM estocástico se aproxima do custo do gradiente verdadeiro usando muito menos do que todo o conjunto de treinamento.\n",
        "\n",
        "Nesse algoritmo estocástico, um novo dataset é obtido por amostrágem randomica sem substitição, inserindo assim um fator de aleatoridade ao sistema. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "ebac_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
